import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix

from sklearn.experimental import enable_halving_search_cv
from sklearn.model_selection import HalvingGridSearchCV

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA

# === 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —ñ –ø–µ—Ä–≤–∏–Ω–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ ===
df = pd.read_csv("diabetes_prediction_dataset.csv")

print("–ü–µ—Ä—à—ñ 5 —Ä—è–¥–∫—ñ–≤:\n", df.head())
print("\n–†–æ–∑–º—ñ—Ä –¥–∞—Ç–∞—Å–µ—Ç—É:", df.shape)
print("\n–¢–∏–ø–∏ –¥–∞–Ω–∏—Ö:\n", df.dtypes)

# –ü—Ä–æ–ø—É—â–µ–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è
print("\n–ü—Ä–æ–ø—É—â–µ–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è:\n", df.isnull().sum())
df.fillna(df.mean(numeric_only=True), inplace=True)

# –î—É–±–ª—ñ–∫–∞—Ç–∏
print("\n–ö—ñ–ª—å–∫—ñ—Å—Ç—å –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤:", df.duplicated().sum())
df.drop_duplicates(inplace=True)

# –û–ø–∏—Å–æ–≤–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
print("\n–û–ø–∏—Å–æ–≤–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\n", df.describe())

# –ö–æ–¥—É–≤–∞–Ω–Ω—è –∫–∞—Ç–µ–≥–æ—Ä—ñ–∞–ª—å–Ω–∏—Ö –∑–º—ñ–Ω–Ω–∏—Ö
df = pd.get_dummies(df, drop_first=True)

# === 2. –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è ===
X = df.drop("diabetes", axis=1)
y = df["diabetes"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

models = {
    'LogisticRegression': LogisticRegression(),
    'RidgeClassifier': RidgeClassifier(),
    'SGDClassifier': SGDClassifier(),
    'SVC': SVC()
}

param_grid = {
    'LogisticRegression': {'C': [0.01, 0.1, 1, 10]},
    'RidgeClassifier': {'alpha': [0.01, 0.1, 1, 10]},
    'SGDClassifier': {'alpha': [0.0001, 0.001, 0.01], 'loss': ['hinge', 'log_loss']},
    'SVC': {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}
}

best_models = {}

for name in models:
    print(f"\nüîç –ü—ñ–¥–±—ñ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –¥–ª—è {name}...")
    search = HalvingGridSearchCV(models[name], param_grid[name], cv=5, n_jobs=-1)
    search.fit(X_train, y_train)
    best_models[name] = search.best_estimator_
    print(f"‚úÖ –ù–∞–π–∫—Ä–∞—â—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–ª—è {name}: {search.best_params_}")

for name, model in best_models.items():
    print(f"\n=== üìä –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ–π–Ω–∏–π –∑–≤—ñ—Ç –¥–ª—è {name} ===")
    y_pred = model.predict(X_test)
    print(classification_report(y_test, y_pred))
    print("–ú–∞—Ç—Ä–∏—Ü—è –ø–ª—É—Ç–∞–Ω–∏–Ω–∏:\n", confusion_matrix(y_test, y_pred))

print("\nüéØ –í–∏–ø–∞–¥–∫–æ–≤—ñ 10 –ø—Ä–æ–≥–Ω–æ–∑—ñ–≤ (LogisticRegression):")
random_indices = np.random.choice(len(X_test), size=10, replace=False)
for i in random_indices:
    x_row = X_test[i].reshape(1, -1)
    true_val = y_test.iloc[i]
    pred_val = best_models['LogisticRegression'].predict(x_row)[0]
    print(f"–Ü–Ω–¥–µ–∫—Å {i} ‚Äî –°–ø—Ä–∞–≤–∂–Ω—ñ–π: {true_val}, –ü—Ä–æ–≥–Ω–æ–∑: {pred_val}")

# === 3. –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—è ===
X_clust = df.drop("diabetes", axis=1)

# –ú–µ—Ç–æ–¥ –ª—ñ–∫—Ç—è
inertia = []
range_n = range(2, 11)
for k in range_n:
    km = KMeans(n_clusters=k, random_state=42)
    km.fit(X_clust)
    inertia.append(km.inertia_)

plt.figure(figsize=(8, 5))
plt.plot(range_n, inertia, marker='o')
plt.title('–ú–µ—Ç–æ–¥ –ª—ñ–∫—Ç—è')
plt.xlabel('–ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤')
plt.ylabel('Inertia')
plt.grid(True)
plt.show()

# –ú–µ—Ç–æ–¥ —Å–∏–ª—É–µ—Ç—ñ–≤
silhouette_scores = []
for k in range_n:
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(X_clust)
    score = silhouette_score(X_clust, labels)
    silhouette_scores.append(score)

plt.figure(figsize=(8, 5))
plt.plot(range_n, silhouette_scores, marker='s', color='green')
plt.title('–ú–µ—Ç–æ–¥ —Å–∏–ª—É–µ—Ç—ñ–≤')
plt.xlabel('–ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤')
plt.ylabel('Silhouette Score')
plt.grid(True)
plt.show()

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ (—á–µ—Ä–µ–∑ PCA)
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_clust)

kmeans = KMeans(n_clusters=3, random_state=42)  # –ê–±–æ —ñ–Ω—à—É –æ–ø—Ç–∏–º–∞–ª—å–Ω—É k
labels = kmeans.fit_predict(X_clust)

plt.figure(figsize=(8, 6))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis', s=40)
plt.title("–í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ (PCA)")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.grid(True)
plt.show()
