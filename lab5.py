import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.experimental import enable_halving_search_cv
from sklearn.model_selection import HalvingGridSearchCV

# 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —ñ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π –∞–Ω–∞–ª—ñ–∑
df = pd.read_csv(r"D:\labl ME\diabetes_prediction_dataset.csv")



print("–ü—Ä–æ–ø—É—â–µ–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è:\n", df.isnull().sum())

# –ö–æ–¥—É–≤–∞–Ω–Ω—è –∫–∞—Ç–µ–≥–æ—Ä—ñ–∞–ª—å–Ω–∏—Ö –∑–º—ñ–Ω–Ω–∏—Ö
df = pd.get_dummies(df, drop_first=True)

# 2. –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –Ω–∞ –æ–∑–Ω–∞–∫–∏ —Ç–∞ —Ü—ñ–ª—å–æ–≤—É –∑–º—ñ–Ω–Ω—É
X = df.drop("diabetes", axis=1)
y = df["diabetes"]

# 3. –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –Ω–∞ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω—ñ —ñ —Ç–µ—Å—Ç–æ–≤—ñ –≤–∏–±—ñ—Ä–∫–∏
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 4. –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 5. –ü–æ–±—É–¥–æ–≤–∞ –º–æ–¥–µ–ª–µ–π
models = {
    'LogisticRegression': LogisticRegression(),
    'RidgeClassifier': RidgeClassifier(),
    'SGDClassifier': SGDClassifier(),
    'SVC': SVC()
}

# 6. –ü—ñ–¥–±—ñ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ HalvingGridSearchCV
param_grid = {
    'LogisticRegression': {'C': [0.01, 0.1, 1, 10]},
    'RidgeClassifier': {'alpha': [0.01, 0.1, 1, 10]},
    'SGDClassifier': {'alpha': [0.0001, 0.001, 0.01], 'loss': ['hinge', 'log_loss']},
    'SVC': {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}
}

best_models = {}

for name in models:
    print(f"\nüîç –ü—ñ–¥–±—ñ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –¥–ª—è {name}...")
    search = HalvingGridSearchCV(models[name], param_grid[name], cv=5, n_jobs=-1)
    search.fit(X_train, y_train)
    best_models[name] = search.best_estimator_
    print(f"‚úÖ –ù–∞–π–∫—Ä–∞—â—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–ª—è {name}: {search.best_params_}")

# 7. –û—Ü—ñ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π
for name, model in best_models.items():
    print(f"\n=== üìä –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ–π–Ω–∏–π –∑–≤—ñ—Ç –¥–ª—è {name} ===")
    y_pred = model.predict(X_test)
    print(classification_report(y_test, y_pred))
    print("–ú–∞—Ç—Ä–∏—Ü—è –ø–ª—É—Ç–∞–Ω–∏–Ω–∏:\n", confusion_matrix(y_test, y_pred))

# 8. –í–∏–ø–∞–¥–∫–æ–≤—ñ 10 –∑–∞–ø–∏—Å—ñ–≤ –∑ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è–º
print("\nüéØ 10 –≤–∏–ø–∞–¥–∫–æ–≤–∏—Ö –ø—Ä–æ–≥–Ω–æ–∑—ñ–≤:")
random_indices = np.random.choice(len(X_test), size=10, replace=False)
for i in random_indices:
    x_row = X_test[i].reshape(1, -1)
    true_val = y_test.iloc[i]
    pred_val = best_models['LogisticRegression'].predict(x_row)[0]
    print(f"–Ü–Ω–¥–µ–∫—Å {i} ‚Äî –°–ø—Ä–∞–≤–∂–Ω—ñ–π: {true_val}, –ü—Ä–æ–≥–Ω–æ–∑–æ–≤–∞–Ω–∏–π (LogisticRegression): {pred_val}")
